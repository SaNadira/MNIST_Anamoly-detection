# MNIST_Anamoly-detection
Unsupervised Anamoly Detection

models  asked for detautoe
So, what are these models and how does they work?

Since anomaly detection can spot trends or departures from expected behavior in data, it is an essential tool in many industries, such as banking, cybersecurity, and healthcare. Principal Component Analysis (PCA) is an effective technique for detecting anomalies concealed in datasets, among the many other anomaly detection techniques available. A dimensionality reduction method called PCA makes it easier to transform complicated data into a lower-dimensional space while keeping the most important information. PCA uses the data’s inherent structure to detect outliers or anomalies by examining residual errors after transformation.
Learning Objectives
Understanding Anomalies, their types, and Anomaly Detection(AD)
Understanding Principal Component Analysis(PCA)
Learning how to use PCA for Anomaly Detection
Implementation of PCA on a dataset for AD
But the real problem is: How can I use pca??
well, as you know, initializing parameters is a crucial process for advancing loss scores and errors, to such states, we can settle our model to detect anamolies.
As you can see in pca section, I tested various amounts of n_components ,but at the end of the day, n = 50 beated others.
the most important reasons are as following
1- Balance Between preserved variance and noise reduction 
2-Enhanced Reconstruction Error gap


GMM(Guasssian Mixture Model):
algorithm:
Gaussian Mixture Model (GMM)
A Gaussian Mixture Model represents a probability distribution as a mixture of multiple Gaussian (normal) distributions. Each Gaussian component in the mixture represents a cluster of data points with similar characteristics. Thus, GMMs work using the assumption that the samples within a dataset can be modeled using different Gaussian distributions.
Anomaly detection using GMM involves identifying data points with low probabilities. If a data point has a significantly lower probability of being generated by the mixture model compared to most other data points, it is considered an anomaly (this will output of a high anomaly score).
Looking for another anomaly detection technique? See Anomaly detection (K-Means)
GMM has some overlap with K-means, however, K-means clusters are always circular, spherical or hyperspherical when GMM can model elliptical clusters.
Features importance (optional)
In most of our DSP blocks, you have the option to calculate the feature importance. Edge Impulse Studio will then output a Feature Importance list that will help you determine which axes generated from your DSP block are most significant to analyze when you want to do anomaly detection.
See Processing blocks > Feature importance
Setting up the Anomaly Detection (GMM) learning block
The GMM anomaly detection learning block has two adjustable parameters: the Number of components and The axes.
Number of components
The number of (gaussian) components can be interpreted as the number of clusters in Gaussian Mixture Models.So , we can deffinately be sure, that n can be number of seperated data we know as prior information of our dataset.
the charts are given in code section.

Autoencoder:
well,well , well. here we go for surfing neural network as our anamoly detector.
algorithm:
What is an AutoEncoder?
An AutoEncoder is a type of neural network used for unsupervised learning. Its
primary function is to learn a compressed representation of input data. An
AutoEncoder consists of two main parts: the encoder and the decoder.
Encoder: This part of the network compresses the input into a latent-space
representation. It encodes the input data as an encoded (compressed)
representation in a reduced dimension.
Decoder: The decoder part aims to reconstruct the input data from the
encoded representation. It tries to generate an output that is as close as
possible to the original input.
The key idea is that AutoEncoders are trained to minimize reconstruction errors,
which makes them efficient in learning the distribution of the input data.
AutoEncoders for Anomaly Detection
In the context of anomaly detection, AutoEncoders are particularly useful. They are
trained on normal data to learn the representation of the normal state. During
inference, if an input significantly deviates from this learned representation, the
AutoEncoder will likely reconstruct it poorly. This poor reconstruction is a signal of
an anomaly.
How It Works
1.  Training: The AutoEncoder is trained exclusively on normal data. The training
process involves adjusting the weights to minimize the reconstruction error.
2.  Inference: During inference, we feed new data to the Autoencoder. If the data is
normal, the AutoEncoder will successfully reconstruct it with minimal error.
However, if the data is anomalous, the reconstruction error will be significantly
higher.
3.  Thresholding: We set a threshold for the reconstruction error. If the error
surpasses this threshold, the data point is flagged as an anomaly.
Detecting Anomaly with the MNIST dataset
Here is an example of anomaly detection using the MNIST dataset. In this example,
we’ll consider all 10 digits (0–9) as normal data. For anomalous data, we’ll create a
synthetic image that doesn’t resemble a digit.
Here’s how you can do it:
Load and Preprocess the MNIST Data: We use all the digits as normal data.
Create Anomalous Data: Generate a synthetic image that doesn’t look like a
digit. This could be a random noise image or an image with arbitrary shapes.
Build and Train an AutoEncoder: The AutoEncoder will be trained on the normal
MNIST data.
Evaluate the Model: We’ll calculate the reconstruction loss for both the normal
MNIST data and the synthetic anomalous image. The expectation is that the
reconstruction error will be significantly higher for the anomalous image.





what is best model?
well let's compare our models one more time, consisting on each one's dataset mutation.Other words, we wanna mention their absolute key role.
Key Differences Between PCA, GMM, and Autoencoders for Anomaly Detection
These methods differ fundamentally in their algorithmic approaches to modeling data and detecting anomalies. Here's a breakdown:
. PCA (Principal Component Analysis)
Algorithmic Approach:

Goal: Reduce dimensionality by projecting data onto orthogonal directions of maximum variance.

Anomaly Detection:

Compute reconstruction error after projecting data to a lower-dimensional space.

High reconstruction error → anomaly.
Steps:
Center the data (mean = 0).
Compute covariance matrix.
Eigen-decomposition to find principal components.
Reconstruct data using top-kk components.
Calculate Error=∣∣X−Xreconstructed∣∣2Error=∣∣X−Xreconstructed​∣∣2.
Assumptions:

Normal data lies in a low-dimensional linear subspace.

Anomalies deviate from this subspace.

Strengths:

Fast and computationally efficient.

Works well for linear relationships.

Weaknesses:

Fails to capture non-linear patterns.

Sensitive to scaling and outliers.
. GMM (Gaussian Mixture Model)
Algorithmic Approach:
Goal: Model data as a mixture of Gaussian distributions.
Anomaly Detection:
Compute likelihood of data points under the GMM.
Low probability → anomaly.
Steps:
Initialize kk Gaussian components.
Expectation-Maximization (EM) algorithm:
E-step: Estimate component responsibilities.
M-step: Update means, covariances, and weights.
Score samples using log-likelihood:
log⁡p(x)=log⁡∑i=1kϕiN(x∣μi,Σi)logp(x)=log∑i=1k​ϕi​N(x∣μi​,Σi​).
Assumptions:
Data is generated from a mixture of Gaussians.
Anomalies are low-probability under the learned distribution.
Strengths:
Captures multi-modal distributions.
Probabilistic framework provides uncertainty estimates.
Weaknesses:
Struggles with high-dimensional data.
Requires choosing kk (number of components).
3. Autoencoder
Algorithmic Approach:
Goal: Learn compressed representations of data through neural networks.
Anomaly Detection:
Compare input to reconstructed output.
High reconstruction error → anomaly.
Steps:
Train a neural network with a bottleneck architecture:
Encoder: z=fenc(x)z=fenc​(x) (compression).
Decoder: x^=fdec(z)x^=fdec​(z) (reconstruction).
Use reconstruction error as anomaly score:
Error=∣∣x−x^∣∣2Error=∣∣x−x^∣∣2.
Assumptions:
Normal data can be compressed and reconstructed accurately.
Anomalies cannot be reconstructed well.
Strengths:
Captures non-linear and hierarchical patterns.
Scalable to high-dimensional data (e.g., images).
Weaknesses:
Computationally expensive to train.
Requires careful hyperparameter tuning.
Algorithmic Comparison
Aspect	PCA	GMM	Autoencoder
Model Type	Linear	Probabilistic	Non-linear Neural Network
Training	Eigen-decomposition	EM Algorithm	Backpropagation
Key Metric	Reconstruction Error	Log-Likelihood	Reconstruction Error
When to Use Each Method
PCA:
Quick baseline for linear anomaly detection.
Low computational resources.
Example: Detecting outliers in tabular data with linear relationships.
GMM:
Clustered or multi-modal data.
Probabilistic interpretation needed.
Example: Fraud detection in transactional data with distinct patterns.
Autoencoder:
Complex, high-dimensional data (e.g., images, text).
Non-linear relationships dominate.
Example: Identifying defective products in manufacturing images
PCA fails if anomalies align with principal components.
GMM struggles with high-dimensional sparse data.
Autoencoders require large datasets to avoid overfitting.
Choose PCA for simplicity, GMM for probabilistic clusters, and Autoencoders for complex patterns.
